# -*- coding: utf-8 -*-
"""end-to-end-dog-vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y6YtZgq6zksGhdrgg2rzm2FpgEmm0ZUi

# End to end multi-class dog breed classifiction
this notebook builds and end-to-end multi-class image classifier using tensorflow and tensorflow hub
## 1. Problem
Identifying the breed of a dog given an image of a dog.
## 2. Data
The data we are using are from Kaggle's dog breed identification competiton.
https://www.kaggle.com/competitions/dog-breed-identification/
## 3. Evaluation
is a file with predition probabilities with each dog breed of each test image
## 4. Features
* we are dealing with images (Unstructured data) so it is probably best we use deep learning/ transfer learning
* there are 120 breeds
* there are 10000+ images in train(with labels) and test sets (no labels)

## Getting our workspace ready
* import TensorFLow ✅
* import TensorFLow Hub ✅
* make sure we are using a GPU ✅
"""

# import TensorFlow and Tensorflow)hub into colab
import tensorflow as tf
import tensorflow_hub as hub
print("TF Version:",tf.__version__)
print("TF hub Version:",hub.__version__)

# check for GPU availbility
print("GPU","Available (Yess!!!)"if tf.config.list_physical_devices("GPU")else "not available")

"""## Getting our data ready (turning it into tensors)
with all machine learning models, our data has to be in numerical format
"""

import pandas as pd
labels_csv=pd.read_csv("drive/MyDrive/Dog Vision/labels.csv")
print(labels_csv.describe())
print(labels_csv.head())

# how many images per breed
labels_csv['breed'].value_counts().plot.bar(figsize=(20,5))

labels_csv['breed'].value_counts().median()

# viewing an image
from IPython.display import Image
Image("drive/MyDrive/Dog Vision/train/001513dfcb2ffafc82cccf4d8bbaba97.jpg")

filenames=["drive/MyDrive/Dog Vision/train/"+fname  for fname in labels_csv["id"]+".jpg"]
filenames[:10]

Image(filenames[3])

import os
if len(os.listdir("drive/MyDrive/Dog Vision/train"))==len(filenames):
  print("filenames match the actual number of the files")
else:
  print("filenames do not match actual number of the files")

labels_csv["breed"][9000]

"""after inserting our training image filepaths into a list, lets prepare our labels"""

import numpy as np
labels=labels_csv["breed"]
labels=np.array(labels)
labels

len(labels)

# see if number of labels matches the number of filenames
if len(labels)==len(filenames):
  print("number of labels matches the number of filenames")

# finding the unique label values
unique_breeds=np.unique(labels)
unique_breeds

len(unique_breeds)

# turning a single label to an array of boolens
print(labels[0])
labels[0]==unique_breeds

# turning every label into a bolean arrat
boolean_labels=[label==unique_breeds for label in labels]
boolean_labels

len(boolean_labels)

# turning boolean array into integers ex:
print(labels[0])# original label
print(np.where(unique_breeds==labels[0]))# index where label occurs
print(boolean_labels[0].argmax()) # index where label occurs in boolean array
print(boolean_labels[0].astype(int))# there will be 1 wher th sample occurs

"""### creating a validation set"""

X, Y= filenames, boolean_labels

"""by starting off expermienting with 1000 images and increase as needed"""

# set number of images to use for experimenting
NUM_IMAGES= 1000 #@param {type:"slider",min:1000,max:10000,step:1000}

# splitting the data into train and validation sets

from sklearn.model_selection import  train_test_split

x_train,x_val,y_train,y_val=train_test_split(X[:NUM_IMAGES],
                                             Y[:NUM_IMAGES],
                                             test_size=0.2,
                                             random_state=42)

len(x_train),len(y_train),len(x_val),len(y_val)

"""## preprocessing Images (turning images into tensors)
1. take an image file path as an input.
2. user tensorflow to read the file and save it to a variable
3. turn our image into tensor
4. resize the image to be a shape of (224,224)
5. return the modified image
"""

# convert image to a NumpY array
from matplotlib.pyplot import imread
image= imread(filenames[42])
image

image.max(),image.min()

# convert image to a tensor
tf.constant(image)

"""### building a function to preprocess images"""

IMG_SIZE=224 # Define image size
# creating a function to preprocess images
def process_image(image_path):
  # read image file
  image=tf.io.read_file(image_path)
  #turn the image into numerical tensor with 3 color channel RGB
  image=tf.image.decode_jpeg(image,channels=3)
  # convert the color channel values from 0-255 to 0-1 values
  image=tf.image.convert_image_dtype(image,tf.float32)
  # Resize the image to our desired values
  image=tf.image.resize(image,size=[IMG_SIZE,IMG_SIZE])
  return image

"""## Turning The Data into batches (minibatches)
why Turning our data into batches?
by trying to process 10000+ images in one time..the data might not fit in the memory
by using 32 batch size, it will be more efficient approach

"""

# Create a simple function to return a tuple of tensors(image,label)

def get_image_label(image_path,label):
  return process_image(image_path),label

process_image(X[42]),Y[42]

"""now we have got a way to turn our data into tuples of tensors in this form (image,label), by making a function to turn all of the data  (X,Y) into batches"""

# Define the batch size
BATCH_SIZE=32
# Creating a function to turn data into batches
def create_data_batches(X, Y=None, batch_size=BATCH_SIZE,valid_data=False,test_data=False):
  if test_data:
    print("creating test data batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X)))
    data_batch=data.map(process_image).batch(BATCH_SIZE)
    return data_batch
  elif valid_data:
    print("creating validation data batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(Y)))
    data_batch=data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch
  else:
    print("creating training data batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(Y)))
    data=data.shuffle(buffer_size=len(X))
    data=data.map(get_image_label)
    data_batch=data.batch(BATCH_SIZE)
  return data_batch

# create training and validation data batches
train_data=create_data_batches(x_train,y_train)
valid_data=create_data_batches(x_val,y_val,valid_data=True)

# check out the diffrent attributes of our data batches
train_data.element_spec,valid_data.element_spec

"""## Visualizing Data Batches
our data is now in batches, these data can be a little to hard to comprehend
"""

import matplotlib.pyplot as plt
def show_25_images(images,labels):
  #setup the figure
  plt.figure(figsize=(10,10))
  # loop through 25
  for i in range(25):
    # create subplots 5X5
    ax=plt.subplot(5,5,i+1)
    # Display an image
    plt.imshow(images[i])
    # add the image label as the title
    plt.title(unique_breeds[labels[i].argmax()])
    # turn the grid line off
    plt.axis("off")

train_data

train_images,train_labels=next(train_data.as_numpy_iterator())
train_images,train_labels

# visualize the data in the training batch
show_25_images(train_images,train_labels)

val_images,val_labels=next(valid_data.as_numpy_iterator())
show_25_images(val_images,val_labels)

"""## building a model
before we build the model, there are few things we need to define first:
 * the input shape(our image shape in the form of tensors) of our model
 * the output shape (image labels in the form of Tensors) of our model
 * the URL of the model we want to use

### preparing the input and output shape
"""

INPUT_SHAPE=[None,IMG_SIZE,IMG_SIZE,3]
OUTPUT_SHAPE=len(unique_breeds)

MODEL_URL=

